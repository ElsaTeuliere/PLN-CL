\documentclass[11pt, a4paper]{article}

\usepackage[utf8]{inputenc} 	% L'encodage. Ca peut Ãªtre Ã  remplacer par \usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\def\ques{\noindent{\bf Question : }}
\usepackage[english]{babel}
\usepackage[top=2cm, bottom=2cm, left=2cm, right=2cm]{geometry}		% Les marges
%\usepackage{listings}		% Pour mettre du code dans le fichier, il suffira ensuite d'appeler \begin{lstlisting}
%\lstset{
%language=Python,        	% choix du langage : C, Python, R, PHP...
%basicstyle=\footnotesize\color{mygray},       % taille de la police du code
%numbers=left,                   % placer les numÃ©ros de lignes Ã  droite (right) ou Ã  gauche (left)
%numberstyle=\footnotesize,        % taille de la police des numÃ©ros
%numbersep=7pt,                  % distance entre le code et sa numÃ©rotation
%stepnumber=1,
%tabsize=4
%}
\usepackage{lscape}
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}
\setcounter{theorem}{0}
\usepackage{comment} 
\usepackage{float}
%\usepackage{capt-of}
\usepackage{graphicx}	% Si besoin de bosser sur des images. Si besoin, aller sur le SDZ.
\usepackage{color}
\definecolor{mygray}{rgb}{0.2,0.2,0.2}
\usepackage{multicol}
% Les packages pour mettre des expressions un peu mathÃ©matisÃ©es.
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathrsfs}
%\usepackage{mathtools}
%\usepackage{dsfont}
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}}

\newtheorem{question}{Q}
\title{	
\normalfont \normalsize 
\textsc{M2 PMA -- Elsa TEULIERE -- 2017-2018} \\ [25pt] % Your university, school and/or department name(s)
\vspace{-0.8cm}
\horrule{0.5pt}  % Thin top horizontal rule
\textbf{Estimation of Poisson log-normale distributions using composite likelihood}
\vspace{-0.8cm}
\horrule{0.5pt}  % Thin top horizontal rule
}

\date{}
\author{}
\graphicspath{ {./figures/} }
 

\begin{document}
\maketitle
\vspace{1cm}

\section{Model and notations}
We have $n$ observations of a sample with the covariables, $(x^i)_{i \in \{1,...,n\}}$ and the data we want to analyse $(Y^i)_{i \in \{1,...,n\}}$. For each $i$, $Y^i \in \mathbb{N}^n$ and $x^i \in \mathbb{R}^p$

\subsection{Poisson log-normale distribution}
We suppose that the data follow a Poisson-log-normale distribution with parameters $mu$,$\Sigma$ \cite{aitchison1989multivariate}.\\
This means that given $Z^{i} \sim \mathcal{N}_{d}(0,\Sigma)$, for all $j \in \{1,...,d\}, Y^i_j \mid Z^i_j \sim \mathcal{P}(e^{x^i \mu_j + Z^i_j})$. All the dependency between the variables is encoded in the matrix $\Sigma$.\\
We can remind here some results about the Poisson-log-normale distribution :
\begin{enumerate}
\item The density of a Poisson-log-normale distribution :\\ For $y \in \mathbb{N}^d$, $p_{(\mu,\Sigma \mid X)}(y)=  \int_{\mathbb(R)^d} \prod_{j=1}^{d}f_{exp(X\mu_j+\theta_j)}(y_j)g_{0,\Sigma}(\theta)\mathrm{d}\theta$ with $X$ the covariables, $f_{\mathrm{exp}(X\mu_j+\theta_j)}$ the density function of a Poisson variable of parameters $\mathrm{exp}(X\mu_j+\theta_j)$  and $g_{0, \Sigma}(\theta)$ the density distribution of a $\mathcal{N}_d(0,\Sigma)$.
\item The moments of a Poisson-log-normale distribution : \\
$\mathrm{E}(Y^i_j)=exp(x^i\mu_j+\frac{1}{2}\sigma^2_j)$\\
$\mathrm{Var}(Y^i_j)=exp(x^i\mu_j+\frac{1}{2}\sigma^2_j)+(exp(x^i\mu_j+\frac{1}{2}\sigma^2_j))^2(exp(\sigma^2_j)-1)$\\
$\mathrm(cov)(Y^i_j,Y^i_k)=exp(x^i\mu_j+\frac{1}{2}\sigma^2_j)exp(x^i\mu_k+\frac{1}{2}\sigma^2_k)(exp(\sigma_{jk} - 1)$.
\end{enumerate}

\subsection{Composite likelihood and M-estimators}
\subsubsection{Composite likelihood}
We use the marginal composite likelihood, also called peudo-likelihood (see \cite{varin2011overview}).In our case we have $\mathcal{L}_C(Y^1,..., Y^n,\mu,\sigma \mid X) = \prod_{i=1}^{n} \prod_{k<j} p_{(\mu,\Sigma \mid X)}{(Y^i_j,Y^i_k)}$. As we do with the likelihhod, we can consider the log-composite likelihood :
$\mathcal{CL}(Y^1,..., Y^n,\mu,\sigma \mid X) = \sum_{i=1}^{n} \sum_{k<j} \mathrm{log}(p_{(\mu',\Sigma', \mid X)}(Y^i_j,Y^i_k)$. Where $\mu'=(\mu_j,\mu_k)$ and $ \Sigma' =(\sigma_{l,p})_{(l,p) \in \{j,k\}^2} $. We introduce the notation $\mathrm{H}_{(\mu,\Sigma \mid X)}(Y^i_j,Y^i_k)=\mathrm{log}(p_{(\mu',\Sigma' \mid X)}(Y^i_j,Y^i_k)$. 
\subsubsection{M-estimators theory and composite likelihood}
Following the definition given by Van der Vaart \cite{vaart_1998}, the log-composite likelihood can be considered as a M-estimator. We have $\mathrm{M}_n : \theta \mapsto \sum_{i=1}^{n} \mathrm{m}_n(\theta)= \sum_{k<j} \mathrm{log}(p_{(\mu',\Sigma', \mid X)}(Y^i_j,Y^i_k)$. We recall here the fundamental theorem of the theory of M-estimators :\\
\\
\begin{theorem} \label{ThMest}
Let $(\mathrm{M}_n)$ be a random sequence of functions and $M$ a determinist set of function. If 
\begin{enumerate}
\item $\underset{\theta \in \Theta}{\mathrm{sup}} \mid \mathrm{M}_n(\theta)-\mathrm{M}(\theta) \mid \overset{\mathbb{P}}{\longrightarrow} 0$
\item the maximum $\theta^\ast$ of M is unique.
\end{enumerate}
Then any sequence of estimators $\widehat{\theta}_n$ with $\mathrm{M}_n(\widehat{\theta}_n \geq \mathrm{M}_n(\theta^\ast)-\circ_p(1)$ converges in probability to $\theta^\ast$.
\end{theorem}


\section{Maximum of the composite likelihood}
The composite likelihood allows us to solve the problem separetly for each marginal likelihood. Here we look for the maximum of $\mathrm{H}_{(\mu',\Sigma' \mid X)}(Y_1,Y_2)$ where $Y$ is distributed as the observations. One have : 
\begin{equation}
p_{(\mu',\Sigma' \mid X)}(Y_1,Y_2)=\int_{\mathbb{R}^2}\frac{\mathrm{e}^{Y_1(X \mu_1+z_1)}}{Y_1 !} \frac{\mathrm{e}^{Y_2(X \mu_2+z_2)}}{Y_2 !} \frac{\mathrm{e}^{-\mathrm{exp}(X\mu_1+z_1)}\mathrm{e}^{-\mathrm{exp}(X\mu_2+z_2)}}{2 \pi \sqrt{\mid \mathrm{det}(\Sigma') \mid} } \mathrm{e}^{-\frac{1}{2} (z_1,z_2) \Sigma'^{-1} (z_1,z_2)^T} \mathrm{d}z_1 \mathrm{d}z_2
\end{equation}
We note $ \mathrm{h}_{(\mu',\Sigma' \mid X)}(z_1,z_2,Y_1,Y_2) =\frac{\mathrm{e}^{Y_1(X \mu_1+z_1)}}{Y_1 !} \frac{\mathrm{e}^{Y_2(X \mu_2+z_2)}}{Y_2 !} \frac{\mathrm{e}^{-\mathrm{exp}(X\mu_1+z_1)}\mathrm{e}^{-\mathrm{exp}(X\mu_2+z_2)}}{2 \pi \sqrt{\mid \mathrm{det}(\Sigma') \mid} } \mathrm{e}^{-\frac{1}{2} (z_1,z_2) \Sigma'^{-1} (z_1,z_2)^T} $
\begin{equation}
\mathrm{h}_{(\mu',\Sigma' \mid X)}(z_1,z_2,Y_1,Y_2)= f_{\mathrm{exp}(X \mu_1+z_1)}(Y_1) f_{\mathrm{exp}(X \mu_2+z_2)}(Y_2) g_{0, \Sigma}(z_1,z_2)
\end{equation}

We see here that the dependency in the variables of interest ($ \mu_1$, $\mu_2$, $ \sigma_{1,1}$, $\sigma_{2,2}$, $\sigma_{1,2}$) is split.
\begin{comment}
\subsection{Derivatives of the composite likelihood}
\subsubsection{With respect to $\mu_{1}$/$\mu_{2}$}
$\mu_1$, $\mu_2$ play symetric roles. 
\begin{equation}
\frac{\partial \mathrm{h}_{(\mu',\Sigma'\mid X)}}{\partial \mu_1}(z_1,z_2,Y_1,Y_2) = X (Y_1- \mathrm{e}^{X \mu_1 +z_1}) \mathrm{h}_{(\mu', \Sigma'\mid X)}(z_1,z_2,Y_1,Y_2)
\end{equation}

Derivating under the $ \int$ and composing with the derivative of the logarithm, we have :

\begin{align}
\frac{\partial \mathrm{H}_{(\mu',\Sigma' \mid X)}}{\partial \mu_1}(Y_1,Y_2) & = &  \frac{\int_{\mathbb{R}^2} X(Y_1- \mathrm{e}^{X\mu_1+z_1 }) \mathrm{h}_{(\mu',\Sigma' \mid X)}(z_1,z_2,Y_1,Y_2) \mathrm{d}z_1 \mathrm{d}z_2}{\int_{\mathbb{R}^2}\mathrm{h}_{(\mu',\Sigma' \mid X)}(z_1,z_2,Y_1,Y_2) \mathrm{d}z_1 \mathrm{d}z_2}\\
& = & X ( Y_1- (Y_1 + 1) \frac{p_{(\mu', \Sigma' \mid X)}(Y_1 +1, Y_2)}{p_{(\mu', \Sigma' \mid X)}(Y_1 , Y_2)})
\end{align}

\subsubsection{With respect to $\sigma_{1,1}$/$\sigma_{2,2}$}
In order to derive with respetc to the $\sigma$ variables we have to consider the reciprocal of the $\Sigma'$-matrix. So one have :
\begin{equation}
g_{(0,\Sigma')} (z_1,z_2) = \frac{1}{2 \pi \sqrt{\mid \sigma_{11} \sigma_{22}- \sigma_{12}^2 \mid}} \mathrm{e}^{-\frac{1}{2}\frac{2 \sigma_{12} z_1 z_2 - \sigma_{22} z_1^2 - \sigma_{11} z_2^2}{\sigma_{12}^2-\sigma_{11} \sigma_{22}}}
\end{equation}

Here again $\sigma_{1,1}$ and $\sigma_{2,2}$ play symetric roles.

We note $\frac{(-) \sigma_{22}}{4 \pi (\mid \sigma_{11} \sigma_{22}- \sigma_{12}^2 \mid ) ^{\frac{3}{2}} }$ the derivative of $ \frac{1}{2 \pi \sqrt{\mid \sigma_{11} \sigma_{22}- \sigma_{12}^2 \mid}}$ with respect to $\sigma_{11}$ depending on the sign of $ \sigma_{11} \sigma_{22}- \sigma_{12}^2 $.
\begin{equation}
\frac{\partial g_{(0,\Sigma ')}}{\partial \sigma_{11}} (z_1,z_2) = ( \frac{(-) \sigma_{22}}{2 (\mid \sigma_{12}^2- \sigma_{11} \sigma_{22} \mid)^{\frac{3}{2}}} - ( \frac{\sigma_{12} z_{2} - \sigma_{22} z_{1}}{(\sigma_{12}^2- \sigma_{11} \sigma_{22})})^2 ) g_{(0,\Sigma')}(z_1,z_2)
\end{equation}

So derivating under the $_int$ and composing with the log-function, one have :
\begin{equation}
\frac{\partial \mathrm{H}_{(\mu',\Sigma ' \mid X)}}{\partial \sigma_{11}} (Y_1,Y_2) =\frac{\int_{\mathbb{R}^2}( \frac{(-) \sigma_{22}}{2 (\mid \sigma_{12}^2- \sigma_{11} \sigma_{22} \mid)^{\frac{3}{2}}} - ( \frac{\sigma_{12} z_{2} - \sigma_{22} z_{1} - \sigma_{11} z_2}{\sigma_{12}^2- \sigma_{11} \sigma_{22}})^2 ) \mathrm{h}_{(\mu',\Sigma' \mid X)}(z_1,z_2,Y_1,Y_2) \mathrm{d}z_1 \mathrm{d}z_2} {\int_{\mathbb{R}^2 }\mathrm{h}_{(\mu',\Sigma' \mid X)}(z_1,z_2,Y_1,Y_2) \mathrm{d}z_1 \mathrm{d}z_2}
\end{equation}

\subsubsection{With respect to $\sigma_{12}$}
We use the same notation than in the previous section for the derivative with respect to $\sigma_{12}$ fo the mornalization term of the normale distribution density function.\\
We have :
\begin{equation}
\frac{\partial g_{(0,\Sigma ')}}{\partial \sigma_{12}} (z_1,z_2) = (\frac{(-) \sigma_{12}}{\mid \sigma_{12}^2 - \sigma_{11} \sigma_{22} \mid} + \frac{\sigma_{12} ( 2 \sigma_{11} z_1 + 2 \sigma_{22} z _1 - \sigma_{11} \sigma{22} z_1 z_2}{(\sigma_{12}^2 - \sigma_{11} \sigma_{22})^2})  g_{(0,\Sigma ')}(z_1,z_2)
\end{equation}

So :
\begin{equation}
\frac{\partial \mathrm{H}_{(\mu',\Sigma ' \mid X)}}{\partial \sigma_{12}} (Y_1,Y_2) =\frac{\int_{\mathbb{R}^2}(\frac{(-) \sigma_{12}}{\mid \sigma_{12}^2 - \sigma_{11} \sigma_{22} \mid} + \frac{\sigma_{12} ( 2 \sigma_{11} z_1 + 2 \sigma_{22} z _1 - \sigma_{11} \sigma{22} z_1 z_2}{(\sigma_{12}^2 - \sigma_{11} \sigma_{22})^2}) \mathrm{h}_{(\mu',\Sigma' \mid X)}(z_1,z_2,Y_1,Y_2) \mathrm{d}z_1 \mathrm{d}z_2} {\int_{\mathbb{R}^2 }\mathrm{h}_{(\mu',\Sigma' \mid X)}(z_1,z_2,Y_1,Y_2) \mathrm{d}z_1 \mathrm{d}z_2}
\end{equation}
\end{comment}

\subsection{Using the results of the maximum likelihood for the composite maximum likelihood.}

To maximize the composite likelihood, it is sufficient to maximize all the terms separetly, \textit{i.e.} to maximize for each couple $(j,k) \in \{1,...,d\}$ the quantity $\frac{1}{n} \sum_{i=1}^{n} log(p_{(\mu,\Sigma \mid X)} (Y^i_j,Y^i_k))$. We observe that this quantity is just a regular maximum likelihood. We show that for each couple of variables the maximum-likelihood estimator is consistent. We then use this result in order to show the consistence of the estimator of the composite likelihood.

\begin{comment}
 We will now follow the following path : first we will show that for one couple $(j,k)$ we do estimate the exact parameters. This consists in showing that the considered quantity converge in probability to a function of the parameters, then showing that the model is identifiable and finely showing that, in our case, the maximum likelihood function only have one maximum wich will be our estimator. Once we shew that for one couple, we will use this in order to estimate the composite-maximum-likelihood estimator. We will have to show again the convergence in probability and the identifiability. To find the composite-maximum-likelihood estimator we will maximize component by component and thus construct the maximum likelihood estimator.
\end{comment} 
 

\subsubsection{For one couple of variables} 

Consider a couple of random variables $(Y_j,Y_k)$ of a vector of random variables $Y=(Y_i)_{i \in \{1...n\}}$ following a $\mathcal{PLN}(\mu,\Sigma)$-distribution, with $\mu=(\mu_{i})_{i \in \{1...n\}}$ and $\Sigma= (\sigma_{il})_{(i,l) \in \{1...n\}^2}$. This couple is distributed as a $\mathcal{PLN}(\mu_{jk},\Sigma_{jk})$ law, with $\mu_{jk}=(\mu_j,\mu_k)$ and $\Sigma_{jk}=(\sigma_{jk})$. Since we are considering here covariables, it is a bit more complicated, and we note $\mathcal{PLN}_X(\mu,\Sigma)$ the Poisson-log-normale distribution given the vector of covariables $X$ and $p_(\mu, \Sigma \mid X)$ its density function.

\begin{theorem} \label{Consistence_2}
For each $(j,k) \in \{1...n\}^2$, $j \neq k$, the estimator $(\widehat{\mu}^{jk}_n,\widehat{\Sigma}^{jk}_n)$ constructed by maximizing the log-likelihood of the couple $(Y^i_j,Y^i_k)_{i \in \{1...n\}}$, given by $\sum_{i=1}^{n} \mathrm{log}(p_{(\mu_{jk},\Sigma_{jk} \mid X)}(Y^i_j,Y^i_k))$, is a consistent estimator of the correlation coefficient $\mu_{jk}$ and of the vector of variance-covariance $\Sigma_{jk}$.
\end{theorem}

\begin{proof}

The idea of this proof is to use the theorem \ref{ThMest} for our composite likelihood. The proof will  follow four steps to verify that the assumptions of the theorem are verified. We will first show the convergence in probability of our M-estimator to a function $\mathrm{M}_{(\mu_{jk},\Sigma_{jk})}$ then the existence of a unique maximum for this function $\mathrm{M}_{(\mu_{jk},\Sigma_{jk})}$. To do this we need to steps : at first we show that our model is identifiable and then, using the Kullback-divergence, that it has a maximum. Combinig this two steps will allow us to conclude the existence of a unique maximum. In the last part, we conclude, using the theroem \ref{ThMest} on the consistence of the estimator. \\
\\
\textbf{Step 1 :} Convergence in probability.\\
Using the large number low, we do have that $\frac{1}{n} \sum_{i=1}^{n} \mathrm{log}(p_{(\mu,\Sigma \mid X)}(Y^i_j,Y^i_k))$ converge almost surely and so in probability to $\mathbb{E}_X[\mathbb{E}_{Y \mid X}[ \mathrm{log}(p_{(\mu_{jk},\Sigma_{jk} \mid X)}(Y_j,Y_k))]]$.\\
\\
\textbf{Step 2 :} Identifiability of the model.\\
 We use the general definition of identifiability \cite{rivoirard}, namely that the family of probabilities $\mathcal{P}=\{\mathbb{P}_\theta,\theta \in \Theta\}$ is identifiable if the application $ \theta \mapsto \mathbb{P}_\theta$ is injective.
 In our case $\mathcal{P}=\{(\mathcal{PLN}_X (\mu,\Sigma))_{X _in \mathcal{X}}; (\mu, \Sigma) \in  \mathcal{M}_{p\times n}(\mathbb{R}) \times \mathcal{M}_n(\mathbb{R})\}$. To show the identifiability of the model we will use the fact that two variables having the same distribution have the same moment. Consider $(\mu_{jk},\Sigma_{jk}) \in \mathcal{M}_{p\times 2}(\mathbb{R}) \times \mathcal{M}_2(\mathbb{R})$ and $(\mu'_{jk},\Sigma'_{jk}) \in \mathcal{M}_{p\times 2}(\mathbb{R}) \times \mathcal{M}_2(\mathbb{R})$, so that  for every vector of covariables $X \in \mathcal{X}$,$\mathcal{PLN}_X(\mu_{jk},\Sigma_{jk})\sim\mathcal{PLN}_X(\mu'_{jk},\Sigma'_{jk})$ . We have :
 \begin{equation}
 \mathbb{E}_{(\mu_{jk},\Sigma_{jk} \mid X)}[Y_j]=\mathbb{E}_{(\mu'_{jk},\Sigma'_{jk} \mid X)} [Y_j] 
 \end{equation}
\begin{equation}
 \mathrm{Var}_{(\mu_{jk},\Sigma_{jk} \mid X)}[Y_j]=\mathrm{Var}_{(\mu'_{jk},\Sigma'_{jk} \mid X)} [Y_j] \\
\end{equation}
\begin{equation}
 \mathrm{Cov}_{(\mu_{jk},\Sigma_{jk} \mid X)}[Y_j,Y_k]=\mathrm{Cov}_{(\mu'_{jk},\Sigma'_{jk} \mid X)} [Y_j,Y_jk]
\end{equation}
Using the formula of this moment we recall in the introduction, we find that $\sigma_{jj} = \sigma'_{jj}$, $\sigma_{jk}=\sigma'_{jk}$ and $X (\mu_j-\mu'_j)=0$. The last condition has to be true for any vector of covariable $X$. We deduce from this that,  we expect $\mathcal{X}^\perp = \{0\}$ for the model to be identifable, \textit{ie.} that $\mathrm{rg}(\mathcal{X})=p$.\\
\\
\textbf{Step 3 :} Existence of a unique maximum for the $\mathrm{M}(\mu_{jk},\Sigma_{jk})$-function.\\
What we consider is nothing else than the log-likelihood of two variables with a Poisson-log-normale distribution. We just shew that the model is identifiable. So there exist one and only one set of parameters $(\mu_{jk}^\ast, \Sigma_{jk}^\ast)$ such that $(Y_j,Y_k) \sim \mathcal{PLN}_X(\mu_{jk}^\ast,\Sigma_{jk}^\ast)$. We first can show that maximizing the log-likelihood is equivalent to minimize the Kullback divergence. We recall that the Kullback divergence for top distribution of density functions $p_\theta$ a,d $p_{\theta^\ast}$ is given by $\mathrm{D_{KL}}(p_{\theta^\ast} \parallel p_{\theta})= \int p_{\theta^\ast}(x) \mathrm{log}(\frac{p_{\theta^\ast}(x)}{p_{\theta}(x)}) \mathrm{d}x$. Since we have $\mathbb{E}_X[\mathbb{E}_{(\mu^\ast,\Sigma^\ast \mid X)}[\mathrm{log}(p_{(\mu_{jk}^\ast,\Sigma_{jk}^\ast \mid X)}(Y_j,Y_k)) - \mathrm{log}(p_{(\mu_{jk},\Sigma_{jk} \mid X)}(Y_j,Y_k))]] = \mathbb{E}_X[\mathrm{D_{KL}}(p_{(\mu_{jk}^\ast,\Sigma_{jk}^\ast \mid X)}\parallel p_{(\mu_{jk},\Sigma_{jk} \mid X)})]$. Since the Kullback-divergence is always positive, we see that maximizing the log-likelihood is equivalent to minimize the Kullback-divergence. The Kullback-divergence is only zero if the two distributions are the same. So we see that the M function is maximum for $(\mu_{jk},\Sigma_{jk})=(\mu_{jk}^\ast,\Sigma_{jk}^\ast)$. So we can conclude that the function M only has one maximum, and this maximum is obtained for the parameters we are interested in.\\
\\
\textbf{Step 4 :} Conclusion.\\
To summurize we have :
\begin{enumerate}
\item For any set of parameters $(\mu_{jk},\Sigma_{jk})$,\\
 $\mid \frac{1}{n} \sum_{i=1}^{n} \mathrm{log}(p_{(\mu,\Sigma \mid X)}(Y^i_j,Y^i_k)) - \mathbb{E}_X[\mathbb{E}_{Y\mid X}[\mathrm{log}(p_{(\mu_{jk},\Sigma_{jk} \mid X)}(Y_j,Y_k))]] \mid \overset{\mathbb{P}}{\longrightarrow} 0 $ 
\item The function $(\mu_{jk},\Sigma_{jk})\mapsto \mathbb{E}_X[\mathbb{E}_{Y\mid X}[\mathrm{log}(p_{(\mu_{jk},\Sigma_{jk} \mid X)}(Y_j,Y_k))]] $ only has one maximum for $(\mu_{jk},\Sigma_{jk})=(\mu^\ast_{jk},\Sigma^\ast_{jk})$, the parameters of the Poisson-log-Normale distribution of the variables $(Y_i,Y_j)$.
\end{enumerate}
Applying theorem \ref{ThMest}, we conclude that the sequence of estimators $(\widehat{\mu}^{jk}_n, \Sigma^{jk}_n)_{n \in \mathbb{N}}$ converges in probability to what we hope to approximate, namely $(\mu^\ast_{jk}, \Sigma^\ast_{jk}$).
\end{proof}

\subsubsection{Generalization to the composite likelihood.}
What we wnat to show is the consistence of the estimators obtained by maximizing the composite likelihood. We recall that the composite likelihood is given by : $\mathcal{CL}_{(\mu,\Sigma) \mid X}(Y^i)=\sum_{i=1}^{n} \sum_{j<k} log(p_{(\mu_{jk},\Sigma_{jk} \mid X)} (Y^i_j,Y^i_k)$.\\
\begin{theorem}
The estimator $(\widehat{\mu}_n,\widehat{\Sigma}_n)$ constructed by maximizing the composite likelihood for $(Y_i)_{i \in \{1...n\}}$ is a consistent estimator of the correlation coefficients $\mu$ and of the vector of variance-covariance $\Sigma$.
\end{theorem}

\begin{proof}
The proof follows exactly the same path that we did for a couple of variables. We will again use theorem \ref{ThMest} and the results of theorem \ref{Consistence_2}.\\
\\
\textbf{Step 1 :} Convergence in probability.
Again using the large number law we have :\\
 $\frac{1}{n}\sum_{i=1}^{n} \sum_{j<k} log(p_{(\mu_{jk},\Sigma_{jk} \mid X)} (Y^i_j,Y^i_k) \overset{\mathbb{P}}{\longrightarrow} \sum_{j<k}\mathbb{E}_X [\mathbb{E}_{Y \mid X}[\mathrm{log}(p_{(\mu_{jk},\Sigma_{jk} \mid X)}(Y_j,Y_k))]]$.\\
 We note M the function $\mathrm{M} : (\mu, \Sigma) \mapsto \sum_{j<k}\mathbb{E}_X [\mathbb{E}_{Y \mid X}[\mathrm{log}(p_{(\mu_{jk},\Sigma_{jk} \mid X)}(Y_j,Y_k))]]$.\\
 \\
 \textbf{Step 2 :} Identifiability of the model.\\
The family of model we consider here is $\mathcal{P}=\{ (\mathcal{PLN}_X(\mu,\Sigma))_{X \in \mathcal{X}}; \mu \in \mathcal{M}_{pn}(\mathbb{R}), \Sigma \in \mathcal{M}_n(\mathbb{R})\}$. Using the moment as we did with a Poisson log-normale distribution of a vector of two variables, but this time for n variables is sufficient to show the identifiability of the model.\\
\\
\textbf{Step 3 :} Existence and uniqueness of the maximum for the function M.\\
Again here we have by the identifiability of the model, under the assumption that the data follow a Poisson log-normale distribution, that there exist one and only one set of parameters $(\mu^\ast, \Sigma^\ast)$ such as given a vector of covariables $X$, $(Y^i) \sim \mathcal{PLN}_X (\mu^\ast, \Sigma^\ast)$. As we did in the previous section we show that maximizing M is equivalent to minimize the function :\\
$(\mu, \Sigma) \mapsto \sum_{j<k} \mathbb{E}_X [\mathrm{D_{KL}}(p_{(\mu^\ast_{jk},\Sigma^\ast_{jk} \mid X)} \parallel p_{(\mu_{jk},\Sigma_{jk} \mid X)}]$.\\
We have here a finite sum of positives variables, so in order to minimize it, we can minimize each of the terms. So the above function is minimal for $\mu^\ast=(\mu_j^\ast)_{j \in \{1...n\}}$ and $\Sigma^\ast$ defined as follow : the term $(\sigma_{jk})_{j \neq k}$ of $\Sigma^\ast$ is the term $\sigma_{12}$ of $\Sigma^\ast_{jk}$  and the term $\sigma_{jj}$ of $\Sigma^\ast$ is the term $\sigma_{11}$ of $\Sigma^\ast_{jk}$.\\
Thus we have the existence of a maximum and its uniqueness.\\
\\
\textbf{Step 4 :} Conclusion.\\
Applying theorem \ref{ThMest}, we conclude that the sequence of estimators $(\widehat{\mu}_n, \widehat{\Sigma}_n)_{n \in \mathbb{N}}$ maximizing the composite likelihood function $(\mu,\Sigma) \mapsto \mathcal{CL}_{(\mu,\Sigma) \mid X}((Y^i)_{i \in \{1..n\}})$  converges in probability to what we hope to approximate, namely $(\mu^\ast, \Sigma^\ast)$, the parameters of our Poisson log-normale model.
\end{proof}

\section{Approximation of the estimators}
We now know that the estimators we consider are consitent, so it now worth to calculate them. To do so, we will use numerical approximations and so we have to calculate their gradients. As we saw previously, in order to estimate the maximum of the compsite likelihood, it is sufficient to estimate the maximum of the log-likelihood for each couple of variables. In order to do this, we have to calculate the gradient of the esitimators. Given a couple $(j,k)$, we will calculate the derivative of it with respect to all the parameters.
\subsection{With respect to the vector $\mu$.}
Let consider $i \in \{1...n\}$. We want to calculate $\partial_{\mu_i}\mathrm{log}( p_{(\mu_{jk},\Sigma_{jk} \mid X)}(Y_j,Y_k))$.
\begin{align*}
\partial_{\mu_i}\mathrm{log}( p_{(\mu_{jk},\Sigma_{jk} \mid X)}(Y_j,Y_k)) &= \frac{\partial_{\mu_i}p_{(\mu_{jk},\Sigma_{jk} \mid X)}(Y_j,Y_k) }{p_{(\mu_{jk},\Sigma_{jk} \mid X)}(Y_j,Y_k)}
\end{align*}

Since :
\begin{align*}
\partial_{\mu_i}p_{(\mu_{jk},\Sigma_{jk} \mid X)}(Y_j,Y_k) &= \mathbb{E}_{(Z_j,Z_k)}[\partial_{\mu_i}p_{(\mu_{jk},\Sigma_{jk} \mid X)}(Y_j,Y_k \mid Z_j,Z_k)]\\
&= \mathbb{E}_{(Z_j,Z_k)}[\partial_{\mu_i}p_{(\mu_{jk},\Sigma_{jk} \mid X)}(Y_j\mid Z_j)p_{(\mu_{jk},\Sigma_{jk} \mid X)}(Y_k\mid Z_k)]
\end{align*}
if $i \neq j$ and $i \neq k$, this partial derivatives is equal to 0.
Otherwise, taking for example $i=j$, one have :
\begin{align*}
\partial_{\mu_j}p_{(\mu_{jk},\Sigma_{jk} \mid X)}(Y_j,Y_k) &= \mathbb{E}_{(Z_j,Z_k)}[\partial_{\mu_j}p_{(\mu_{jk},\Sigma_{jk} \mid X)}(Y_j\mid Z_j)p_{(\mu_{jk},\Sigma_{jk} \mid X)}(Y_k\mid Z_k)]\\
&= \mathbb{E}_{(Z_j,Z_k)}[X(Y_j p_{(\mu_{jk},\Sigma_{jk} \mid X)} (Y_j \mid Z_j)-(Y_{j}+1) p_{(\mu_{jk},\Sigma_{jk} \mid X)}(Y_j+1\mid Z_j))p_{(\mu_{jk},\Sigma_{jk} \mid X)}(Y_k\mid Z_k)]\\
&= X(Y_j p_{(\mu_{jk},\Sigma_{jk} \mid X)}(Y_j,Y_k)-(Y_j+1)p_{(\mu_{jk},\Sigma_{jk} \mid X)}(Y_j+1,Y_k))
\end{align*}
Where the second equality comes from the fact that $Y_j \mid Z_j$ is following a Poisson-distribution. We have :
\begin{align*}
\partial_{\mu_j} p_{\mu_j,\Sigma_{jj} \mid X}(Y_j \mid Z_j) &= \partial_{\mu_j} (\mathrm{e}^{Y_j (X \mu_j + Z_j)} \mathrm{exp}(-e^{X \mu_j + Z_j}) \frac{1}{Y_j !} )\\
&= X Y_j\frac{\mathrm{e}^{Y_j (X \mu_j + Z_j)} \mathrm{exp}(-e^{X \mu_j + Z_j})}{Y_j !} - X \frac{\mathrm{e}^{(Y_j+1) (X \mu_j + Z_j)} \mathrm{exp}(-e^{X \mu_j + Z_j})}{Y_j !}\\
&= X(Y_j p_{(\mu_{j},\Sigma_{jj} \mid X)} (Y_j \mid Z_j)-(Y_{j}+1) p_{(\mu_{j},\Sigma_{jj} \mid X)}(Y_j+1\mid Z_j))
\end{align*}
So we conclude that
\begin{align*}
\partial_{\mu_i}\mathrm{log}( p_{(\mu_{jk},\Sigma_{jk} \mid X)}(Y_j,Y_k)) &= X(Y_j -(Y_j+1)\frac{p_{(\mu_{jk},\Sigma_{jk} \mid X)}(Y_j+1,Y_k)}{p_{(\mu_{jk},\Sigma_{jk} \mid X)}(Y_j,Y_k))})
\end{align*}
\subsection{With respect to the matrix of variance - covariance.}
We note $\sigma_{i,j}$ the terms of the matrix of varianve covariance. The term of this matrix are only present in the density function of the multivariate normale distribution wth which the parameters of the Poissons are taken. In order to calcul the derivatives we will in both cases follow the same path :
\begin{enumerate}
\item We calculate the derivatives of the density function of the bivariate normale distribution wrt the parameter of interest. By integrating under the $\int$ we deduce the derivative of the Poisson-log-normale density function.
\item We integrate by part to have a nice expression of this derivative.
\end{enumerate}
We recall here that the density function of the Poisson log-Normal distribution with mean $\mu$ and matrix of variance-covariance $\Sigma$, given a vector of covariables $X$ is given by :
\begin{equation*}
p_{(\mu_{jk},\Sigma_{jk} \mid X)}(y_j,y_k)=\int_{\mathbb{R}^2} p_{e^{X\mu_{jk}+z_1}}(y_j) p_{e^{X\mu_{jk}+z_2}}(y_k) h_{(0,\Sigma_{jk})}(z_j,z_k) \mathrm{d}z_j \mathrm{d}z_k
\end{equation*}
With $h_{(0,\Sigma_{jk})}$ the density function of the bivariate normale distribution of mean $0$ and variance-covariance $\Sigma_{jk}$ and $p_{\beta}$ the density function of a Poisson distribution with parameters $\beta$.
We finally recall that : $h_{(0,\Sigma_{jk})}(z_1,z_2)=\frac{1}{2 \pi \sqrt{\mid \sigma_{jj} \sigma_{kk}-\sigma_{jk}^2}\mid} \mathrm{e}^{-\frac{1}{2} \frac{\sigma_{jj} z_1^2 + \sigma_{kk} z_2^2 -2 \sigma_{jk} z_1 z_2}{\sigma_{jj} \sigma_{kk}-\sigma_{jk}^2}}$.

\subsubsection{wrt $\sigma_{jj}$ and $\sigma_{kk}$}
\begin{align*}
\partial_{\sigma_{jj}} h_{(0,\Sigma_{jk})} (z_1,z_2) &= \frac{1}{2} [ \frac{-\sigma_{kk}}{\sigma_{jj} \sigma_{kk}-\sigma_{jk}^2} +  \frac{(\sigma_{jk} z_1 - \sigma_{kk} z_2)^2}{(\sigma_{jj} \sigma_{kk}-\sigma_{jk}^2)^2}  ]h_{(0,\Sigma_{jk})}(z_1,z_2)
\end{align*}

Since the bivariate normale distribution admit second order moments, we can apply  the thoerem of derivation under $\int$ and deduce that :
\begin{align*}
\partial_{\sigma_{jj}} p_{(\mu_{jk},\Sigma_{jk} \mid X)}(y_j,y_k) &=\int_{\mathbb{R}^2} p_{e^{X\mu_{jk}+z_1}}(y_j) p_{e^{X\mu_{jk}+z_2}}(y_k)\frac{1}{2} [ \frac{-\sigma_{kk}}{\sigma_{jj} \sigma_{kk}-\sigma_{jk}^2} +  \frac{(\sigma_{jk} z_1 - \sigma_{kk} z_2)^2}{(\sigma_{jj} \sigma_{kk}-\sigma_{jk}^2)^2}  ] h_{(0,\Sigma_{jk})}(z_j,z_k) \mathrm{d}z_j \mathrm{d}z_k
\end{align*}

\noindent We will now try to find a nicer expression of this derivative. One can remark that $\partial_{z_2} (-\frac{1}{2} \frac{\sigma_{jj} z_1^2 + \sigma_{kk} z_2^2 -2 \sigma_{jk} z_1 z_2}{\sigma_{jj} \sigma_{kk}-\sigma_{jk}^2}) = \frac{\sigma_{jk} z_1 - \sigma_{kk} z_2}{\sigma_{jj} \sigma_{kk}-\sigma_{jk}^2}$.

This gives the clue to integrate by part with respect to $z_2$. We then have :
\begin{align*}
\partial_{\sigma_{jj}} p_{(\mu_{jk},\Sigma_{jk} \mid X)}(y_j,y_k) =& \int_{\mathbb{R}} [  p_{e^{X\mu_{jk}+z_1}}(y_j) p_{e^{X\mu_{jk}+z_2}}(y_k) \frac{\sigma_{jk} z_1 - \sigma_{kk} z_2}{\sigma_{jj} \sigma_{kk}-\sigma_{jk}^2} h_{(0,\Sigma_{jk})}(z_1,z_2) ]_{- \infty}^{+\infty} \mathrm{d}z_1\\
 &- \int_{\mathbb{R}^2} y_k p_{e^{X\mu_{jk}+z_1}}(y_j) p_{e^{X\mu_{jk}+z_2}}(y_k) \frac{\sigma_{jk} z_1 - \sigma_{kk} z_2}{\sigma_{jj} \sigma_{kk}-\sigma_{jk}^2} h_{(0,\Sigma_{jk})}(z_1,z_2) \mathrm{d}z_1 \mathrm{d}z_2\\
 &- \int_{\mathbb{R}^2} (y_k+1) p_{e^{X\mu_{jk}+z_1}}(y_j)  p_{e^{X\mu_{jk}+z_2}}(y_k+1) \frac{\sigma_{jk} z_1 - \sigma_{kk} z_2}{\sigma_{jj} \sigma_{kk}-\sigma_{jk}^2} h_{(0,\Sigma_{jk})}(z_1,z_2) \mathrm{d}z_1 \mathrm{d}z_2
\end{align*}
The derivatives of the density function of the Poisson distribution is obtained as we did in the previous section.\\
The first rht of this equality being null, we can again integrate by part wrt $z_2$ and we get :
\begin{align*}
\partial_{\sigma_{jj}} p_{(\mu_{jk},\Sigma_{jk} \mid X)}(y_j,y_k) &= y_k^2 p_{(\mu_{jk},\Sigma_{jk} \mid X)}(y_j,y_k)\\
& - ((y_k+1)^2 + y_k (y_k+1) ) p_{(\mu_{jk},\Sigma_{jk} \mid X)}(y_j,y_k+1) \\
& + (y_k+1)(y_k+2) p_{(\mu_{jk},\Sigma_{jk} \mid X)}(y_j,y_k+2)
\end{align*} 
We are calculating this gradients in order to approximate the composite log likelihood so we are in fact interested in $\partial_{\sigma_{jj}} \mathrm{log}p_{(\mu_{jk},\Sigma_{jk} \mid X)}(y_j,y_k)$, which we now can easily calculate.
\begin{align*}
\partial_{\sigma_{jj}} p_{(\mu_{jk},\Sigma_{jk} \mid X)}(y_j,y_k) &= y_k^2 - ((y_k+1)^2 + y_k (y_k+1) ) \frac{p_{(\mu_{jk},\Sigma_{jk} \mid X)}(y_j,y_k+1) } {p_{(\mu_{jk},\Sigma_{jk} \mid X)}(y_j,y_k)}\\
& + (y_k+1)(y_k+2)\frac{ p_{(\mu_{jk},\Sigma_{jk} \mid X)}(y_j,y_k+2)}{p_{(\mu_{jk},\Sigma_{jk} \mid X)}(y_j,y_k)}
\end{align*}
Symetrically we have wrt $\sigma_{kk}$ :
\begin{align*}
\partial_{\sigma_{kk}} p_{(\mu_{jk},\Sigma_{jk} \mid X)}(y_j,y_k) &= y_j^2 - ((y_j+1)^2 + y_j (y_j+1) ) \frac{p_{(\mu_{jk},\Sigma_{jk} \mid X)}(y_j+1,y_k) } {p_{(\mu_{jk},\Sigma_{jk} \mid X)}(y_j,y_k)}\\
& + (y_j+1)(y_j+2)\frac{ p_{(\mu_{jk},\Sigma_{jk} \mid X)}(y_j+2,y_k)}{p_{(\mu_{jk},\Sigma_{jk} \mid X)}(y_j,y_k)}
\end{align*}

\subsubsection{wrt $\sigma_{jk}$ :}
\begin{align*}
\partial_{\sigma_{jk}}h_{(0,\Sigma)}(z_1,z_2) &= [\frac{\sigma_{jk}}{\sigma_{jj} \sigma_{kk} - \sigma_{jk}^2} + \frac{-\sigma_{jk} ( \sigma_{jj} z_1^2 + \sigma_{kk} z_2^2 ) + \sigma_{jk} z_1 z_2 + \sigma_{jj} \sigma_{kk} z_1 z_2}{(\sigma_{jj} \sigma_{kk} - \sigma_{jk}^2)^2}] h_{(0,\Sigma)}(z_1,z_2)
\end{align*}
Again we can apply the theorem of derivating under the $\int$. We deduce that :
\begin{align*}
&\partial_{\sigma_{jk}}p_{(\mu_{jk},\Sigma_{jk} \mid X)}(y_j,y_k) =
\int_{\mathbb{R}^2} p_{e^{X\mu_{jk}+z_1}}(y_j) p_{e^{X\mu_{jk}+z_2}}(y_k)\\
&[   \frac{\sigma_{jk}}{\sigma_{jj} \sigma_{kk} - \sigma_{jk}^2} + \frac{-\sigma_{jk} ( \sigma_{jj} z_1^2 + \sigma_{kk} z_2^2 ) + \sigma_{jk} z_1 z_2 + \sigma_{jj} \sigma_{kk} z_1 z_2}{(\sigma_{jj} \sigma_{kk} - \sigma_{jk}^2)^2}] h_{(0,\Sigma_{jk})}(z_j,z_k) \mathrm{d}z_j \mathrm{d}z_k
\end{align*}

One can note that :
\begin{align*}
\partial_{z_1} ( - \frac{1}{2} \frac{\sigma_{jj} z_1^2 + \sigma_{kk} z_2^2 - 2 \sigma_{jk} z_1 z_2 }{\sigma_{jj} \sigma_{kk}-  \sigma_{jk}^2} ) & \partial_{z_2} ( - \frac{1}{2} \frac{\sigma_{jj} z_1^2 + \sigma_{kk} z_2^2 - 2 \sigma_{jk} z_1 z_2 }{\sigma_{jj} \sigma_{kk}-  \sigma_{jk}^2} ) =\\
&  \frac{-\sigma_{jk} ( \sigma_{jj} z_1^2 + \sigma_{kk} z_2^2 ) + \sigma_{jk} z_1 z_2 + \sigma_{jj} \sigma_{kk} z_1 z_2}{(\sigma_{jj} \sigma_{kk} - \sigma_{jk}^2)^2}
\end{align*}
Which gives a clue that we could do again tow part integrations : one towards $z_1$ and one towards $z_2$. \\
We will do a first integration by part with respect to $z_1$. We note that : $ \partial_{z_1} ( \frac{\sigma_{jk}z_1-\sigma_{kk}z_2}{\sigma_{jj} \sigma_{kk}- \sigma_{jk}^2} h_{(0,\Sigma_{jk})}(z_1,z_2) = [\frac{\sigma_{jk}}{\sigma_{jj} \sigma_{kk} - \sigma_{jk}^2} + \frac{-\sigma_{jk} ( \sigma_{jj} z_1^2 + \sigma_{kk} z_2^2 ) + \sigma_{jk} z_1 z_2 + \sigma_{jj} \sigma_{kk} z_1 z_2}{(\sigma_{jj} \sigma_{kk} - \sigma_{jk}^2)^2}] h_{(0,\Sigma)}(z_1,z_2)$. And taking the derivative of the poisson density with respect to its parameters allows us to conclude that :
\begin{align*}
&\partial_{\sigma_{jk}}p_{(\mu_{jk},\Sigma_{jk} \mid X)}(y_j,y_k) = \int_{\mathbb{R}} [ p_{e^{X\mu_{jk}+z_1}}(y_j) p_{e^{X\mu_{jk}+z_2}}(y_k) \frac{\sigma_{jk}z_1-\sigma_{kk}z_2}{\sigma_{jj} \sigma_{kk}- \sigma_{jk}^2} h_{(0,\Sigma_{jk})}(z_1,z_2)]_{- \infty}^{+ \infty} \mathrm{d}z_2\\
& - \int_{\mathbb{R}^2} (y_j p_{e^{X\mu_{jk}+z_1}}(y_j)- (y_j+1) p_{e^{X\mu_{jk}+z_1}}(y_j+1)) p_{e^{X\mu_{jk}+z_2}}(y_k) \frac{\sigma_{jk} z_1 - \sigma_{kk} z_2}{\sigma_{jj} \sigma_{kk} - \sigma_{jk}^2} h_{(0,\Sigma_{jk})}(z_1,z_2) \mathrm{d}z_1  \mathrm{d}z_2
\end{align*}
The first right-hand term of the equality being equal to zero, we have, integrating by part again but this time wrt $z_2$, we have :
\begin{align*}
&\partial_{\sigma_{jk}}p_{(\mu_{jk},\Sigma_{jk} \mid X)}(y_j,y_k) =\int_{\mathbb{R}^2} (y_j p_{e^{X\mu_{jk}+z_1}}(y_j)- (y_j+1) p_{e^{X\mu_{jk}+z_1}}(y_j+1))\\
& (y_k p_{e^{X\mu_{jk}+z_2}}(y_k)-(y_k + 1) p_{e^{X\mu_{jk}+z_2}}(y_k+1))  h_{(0,\Sigma_{jk})}(z_1,z_2) \mathrm{d}z_1  \mathrm{d}z_2
\end{align*}
What we can also write :
\begin{align*}
\partial_{\sigma_{jk}}p_{(\mu_{jk},\Sigma_{jk} \mid X)}(y_j,y_k) = &y_k y_j p_{(\mu_{jk},\Sigma_{jk} \mid X)}(y_j,y_k)-(y_k+1)y_j p_{(\mu_{jk},\Sigma_{jk} \mid X)}(y_j,y_k+1)\\
& -(y_j+1)y_k p_{(\mu_{jk},\Sigma_{jk} \mid X)}(y_j,y_k)+ (y_k+1) (y_j+1)p_{(\mu_{jk},\Sigma_{jk} \mid X)}(y_j+1,y_k+1))
\end{align*}
Again we are interested in $\partial_{\sigma_{jk}}\mathrm{log}p_{(\mu_{jk},\Sigma_{jk} \mid X)}(y_j,y_k)$ which we can now easily calculate by dividing the above quantity by $p_{(\mu_{jk},\Sigma_{jk} \mid X)}(y_j,y_k)$.

\section{Assymptotic normality}
\subsection{Theoric result}
Again, we will here use the results from \cite{vaart_1998}. We have the following theorem.
\begin{theorem}

\end{theorem}
In order to be able to estimate the Godambe matrix we have to calculate the second derivative of the composite likelihood. Using the formula we get in the previous section, one have :
\begin{landscape}
\paragraph{$\partial^2 \mu_i$}
\begin{align*}
\partial^2  \mu_i \mathcal{CL}(Y^1,...,Y^n,\mu,\Sigma \mid X) =& X X^T \sum_{j \neq i} \frac{1}{p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j)^2} [(Y_i+1)(Y_i+2)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i+2,Y_j)\\& - (Y_i+1)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i+1,Y_j)-(Y_i+1)^2 p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i+1,Y_j)^2] 
\end{align*}
\paragraph{$\partial \mu_j \partial \mu_i$ ($i \neq j$)}
\begin{align*}
\partial^2 _{\mu_i \mu_j} \mathcal{CL}(Y^1,...,Y^n,\mu,\Sigma \mid X) = X X^T \frac{(Y_i+1)(Y_j+1)}{p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j)^2} [p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i+1,Y_j+1)p_{(\mu_{ij},\Sigma_{ij})\mid X}(Y_i,Y_j) - p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i+1,Y_j)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j+1)]
\end{align*}
\paragraph{$\partial^2 \sigma_{ii}$}
\begin{align*}
\partial^2 \sigma_{ii}\mathcal{CL}(Y^1,...,Y^n,\mu,\Sigma \mid X) =  \sum_{j \neq i} & \frac{(Y_j+1)}{p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j)^2} [ -(2 Y_j+1)^2 p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j+1)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j)\\
&+(Y_j+2) (4 Y_j^2 + 12 Y_j +7) p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j+2) p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j)\\
&- 2 (Y_j+2)(Y_j+3)(2 Y_j+3) p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j+3)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j)\\
& -(2 Y_j+1)^2 (Y_j+1) p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j+1)^2\\
& + 2(Y_j+1)(Y_j+2)(2Y_j+1)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j+2)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j+1)\\
&+(Y_j+2)(Y_j+3)(Y_j+4)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j+4)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j)\\
&-(Y_j+2)^2(Y_j+1)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j+2)^2]
\end{align*}
\paragraph{$\partial^2 \sigma_{ii} \sigma_{jj}$ ($i \neq j$}
\begin{align*}
\partial^2 \sigma_{ii}\mathcal{CL}(Y^1,...,Y^n,\mu,\Sigma \mid X) = & \frac{(Y_j+1)(Y_i+1)}{p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j)^2} [(2Y_i+1) [ (2Y_j+1) (p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i+1,Y_j+1)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j) - p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i+1,Y_j)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j+1) )\\
& + (Y_j+2) (p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j+2)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i+1,Y_j)- p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i+1,Y_j+2) )]\\
& + (Y_i+2) [ (2Y_j+1) p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j+1)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i+2,Y_j)-p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i+2,Y_j+2)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j) )\\
& +(Y_j+2) ( p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j) p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i+2,Y_j+2)-p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j+2)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i+2,Y_j) ) ] ]
\end{align*}
\paragraph{$\partial \sigma_{ii} \mu_{ii}$}
\begin{align*}
\partial^2 \sigma_{ii} \mu_{ii} \mathcal{CL}(Y^1,...,Y^n,\mu,\Sigma \mid X) =  X \sum_{j \neq i} & \frac{(Y_i+1)(Y_j+1)}{p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j)^2} [(2Y_j+1) (p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i+1,Y_j+1)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j)-p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j+1)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i+1,Y_j))\\
& + (Y_j+2) (p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j+2)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i+1,Y_j)-p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i+1,Y_j+2)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j) ) ]
\end{align*}
\paragraph{$\partial \sigma_{ii} \mu{jj}$ ($i \neq j$)}
\begin{align*}
\partial^2 \sigma_{ii} \mu_{jj} \mathcal{CL}(Y^1,...,Y^n,\mu,\Sigma \mid X) =& X \frac{(Y_j+1)}{p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j)^2}[-(2Y_j+1) p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j+1)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j)\\
& + (Y_j+2) p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j) [(2Y_j+3)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j+2)-(Y_j+3) p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j+3)] \\
&+ (Y_j+1) p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j+1)[(Y_j+2)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j+2)-(2Y_j+1)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j+1)]]
\end{align*}
\paragraph{$\partial^2\sigma_{ij}$}
\begin{align*}
\partial^2 \sigma_{ij}  \mathcal{CL}(Y^1,...,Y^n,\mu,\Sigma \mid X) =& \frac{1}{p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j)^2}[-(Y_j)^2(Y_j+1)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j+1)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j)\\
&+(Y_i+1)(Y_j+1)(2Y_iY_k+2Y_i+2Y_j+1)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i+1,Y_j+1)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j)\\
&+(Y_i)^2(Y_j+1)(Y_j+2)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j+2)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j)\\
&-(Y_j+1)(Y_j+2)(Y_i+1)(2Y_i+1)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i+1,Y_j+2)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j)\\
&-Y_i^2(Y_j+1)^2 p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j+1)^2 - 2 Y_iY_j(Y_i+1-Y_j+1)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j+1)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i+1,Y_j)\\
&+2 Y_i (Y_j+1)^2(Y_i+1)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i+1,Y_j+1)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j+1)\\
&-Y_j^2(Y_k+1)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i+1,Y_j)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j)\\
&+Y_j^2(Y_i+1)(Y_i+2)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i+2,Y_j)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j)-(Y_i+1)^2Y_j^2 p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i+1,Y_j)^2\\
&+2(Y_i+1)^2Y_j(Y_j+1)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i+1,Y_j+1)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i+1,Y_j)\\
&-(Y_i+1)(Y_j+1)(Y_i+2)(2Y_j+1)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i+2,Y_j+1)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j)\\
&(Y_i+1)(Y_i+2)(Y_j+1)(Y_j+2)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i+2,Y_j+2)p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i,Y_j)-(Y_i+1)^2(Y_j+1)^2p_{(\mu_{ij},\Sigma_{ij} \mid X)}(Y_i+1,Y_j+1)^2
\end{align*}
\end{landscape}
\section{Application to spatial models}
\subsection{Spatial models}
We will here reverse the problem. We considere the case in which we have one specie but several sampling sites. The aime is to use or PNL-model in order to estimate the dependance-structure between the sites. \\
\\
As it is usually the case in spatial models \cite{cressie2015statistics} we assume the matrix of varince-covariance to be in the form :
\begin{align*}
\Sigma = \sigma^2 \begin{pmatrix}
1 &  e^{-\alpha d_{1,2}} &  e^{-\alpha d_{1,2}} & \ldots &  e^{- \alpha d_{1,n}}  \\
 e^{-\alpha d_{1,2}} & 1 & \ldots & \ldots &  e^{- \alpha d_{2,n}}\\
 \vdots & & \ddots & & \vdots\\
 \vdots & & & \ddots & \vdots \\
 e^{-\alpha d_{1,n}} &  e^{-\alpha d_{2,n}} & \ldots & \ldots & 1 
\end{pmatrix}
\end{align*}
avec $ (\sigma, \alpha) \in \mathbb{R}_{+}$.
The idea here is to adapt our PLN model in order to estimate $\sigma^2$ and $\alpha$. 

\subsection{Sampling planning and calculation costs}

The calculation costs the maximum-likelihood estimators is very high, especially when the number of sampled sites is high. Our idea is to maximize the composite-likelihood only over a finite number of couples, \textit{ie} to estimate the parameters by maximizing the $\mathrm{CL}^{I} = \sum_{(i,j) \in I} log( p_{\theta}(Y_i,Y_j))$, with $I$ a subset of $\{(j,k) \in \{1...n\}^2\}$. The question is, how to choose an optimal subset $I$.\\
\\
Note $\overhat{\sigma_{ij}}$ an estimator of the parameter of covariance $\sigma_{ij}$.
One have, by a linear model :
\begin{align*}
\mathrm{ln}(\overhat{\sigma_{ij}} = mathrm{ln} ( \sigma^2) - \alpha d_ij + E_{ij}
\end{align*}
With $E_{ij}$ a noise due to estimation. We estime this noise to be  identically distributed  for all the couples considered.
A classical result gives that the variance of the regression is given by : 
\bibliographystyle{plain}
\bibliography{Biblio.bib}
\end{document}