\documentclass[11pt, a4paper]{article}

\usepackage[utf8]{inputenc} 	% L'encodage. Ca peut ÃƒÂªtre Ãƒ  remplacer par \usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\def\ques{\noindent{\bf Question : }}
\usepackage[english]{babel}
\usepackage[top=2cm, bottom=2cm, left=2cm, right=2cm]{geometry}		% Les marges
%\usepackage{listings}		% Pour mettre du code dans le fichier, il suffira ensuite d'appeler \begin{lstlisting}
%\lstset{
%language=Python,        	% choix du langage : C, Python, R, PHP...
%basicstyle=\footnotesize\color{mygray},       % taille de la police du code
%numbers=left,                   % placer les numéros de lignes Ãƒ  droite (right) ou Ãƒ  gauche (left)
%numberstyle=\footnotesize,        % taille de la police des numéros
%numbersep=7pt,                  % distance entre le code et sa numérotation
%stepnumber=1,
%tabsize=4
%}
\usepackage{lscape}
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\setcounter{theorem}{0}
\usepackage{comment} 
\usepackage{float}
%\usepackage{capt-of}
\usepackage{graphicx}	% Si besoin de bosser sur des images. Si besoin, aller sur le SDZ.
\usepackage{color}
\definecolor{mygray}{rgb}{0.2,0.2,0.2}
\usepackage{xcolor}
\usepackage{multicol}
% Les packages pour mettre des expressions un peu mathématisées.
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathrsfs}
%\usepackage{mathtools}
%\usepackage{dsfont}
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}}
\newcommand{\SR}[2]{\textcolor{gray}{#1}\textcolor{blue}{#2}}


\newtheorem{question}{Q}
\title{	
Composite likelihood inference of the parameters of a Poisson log-normale distribution
}

\date{\today}
\author{Elsa TEULIERE}

\graphicspath{ {./figures/} }
 

\begin{document}
\maketitle
\vspace{1cm}
\section*{Introduction}
\section{Poisson log-normal model}
The Poisson log normal model was developped by Aitchison \textit{et al.}\cite{aitchison1989multivariate}, to modell multivariate count data.  The idea is to introduce latent gaussian variables encoding the dependency between the variables. In this section we introduce this model and our notation and briefly review the existing litterature on this model.
\subsection{Definitions and notations}
We note $\mathcal{N}(p,Q)$ the multivariate normale distribution of mean $p$ and variance-covariance $Q$, and $\mathcal{P}(\alpha)$ the Poisson-distribution of mean $\alpha$.
\subsubsection{The Model} 
We consider $(\mu,\Sigma) \in \mathbb{R}^n \times \mathcal{M}_n(\mathbb{R})$, with $\Sigma$ a symetric non negative matrix, the parameters of the model.\\
We consider the latent variables $Z \sim \mathcal{N}(0,\Sigma)$.\\
Multivariate count data $(Y_1,...,Y_n)$ follow a Poisson log-normal model of mean $\mu$ and matrix of variance-covariance $\Sigma$ if :
\begin{center}
$\forall i \in \{1,...n\}, Y_i \mid Z_i \sim \mathcal{P}(e^{\mu_i+Z_j})$
\end{center}
and 
\begin{center}
$\forall (i,j) \in \{1,...,n\}^2$, $ Y_i\mid Z_i$ and $Y_j\mid Z_j$ independant.
\end{center}

We typically consider $N$ independant sampling experiment of $n$ variables each time, for example the case of sampling $n$ species over $N$ sites. In our notations, the exposant allways stands for the sampling experiment, and the underscore stand for the observed variable.\\
So we now have $(Z^j)_{j \in \mathbb{N}}$ latent variables, independant and identically distributed : $\forall j \in \{1,...,N\}$, $Z^j \sim \mathcal{N}(0,\Sigma)$, and :
\begin{center}
$\forall (i,j) \in \{1,...,n\} \times \{1,...,N\}$, $Y^j_i \mid Z^j_i \sim \mathcal{P}(e^{\mu_i+Z^j_i})$.
\end{center}
The advantage of this model is that all the dependance structure is encoded through gaussian variables.
\subsubsection{Extension accounting for covariates and offset}
In multivariate data, the dependency can occur through covariates. As developped in \cite{chiquet2017variational}, we will postulate the existence of a linear regression in  the parameters space. \\
\\
In this case, we consider a vector $X^j \in \mathbb{R}^d$ of covariates (collected for each sampling). Let $M$ be a $d \times n$ Matrix, called the matrix of regression parameters. We note $\mu^i$ the $i^{th}$ column of the matrix M. We can also add one offset parameter per observation $O^j_i$. The offset, can typically be the sampling effort. In this case the Poisson log-normal model is given by :
\begin{center}
$\forall (i,j) \in \{1,...,n\} \times \{1,...,N\}$, $Y^j_i \mid Z^j_i \sim \mathcal{P}(e^{O^j_i + X_j^T \mu^i +Z^j_i})$ 
\end{center}
The set of parameters, that we aim to estimate is now given by $(M,\Sigma) \in \mathcal{M}_{d \times n}(\mathbb{R}) \times \mathcal{M}_{n}(\mathbb{R})$. $M$ is the matrix of regression parameter. Each column of $M$ contain all the regression parameters toward the covariables for one observed variable. We will now use the following notations : $X \in \mathcal{M}_{d \times N} ( \mathbb{R})$ stands for the matrix of covariates. The $j^{th}$ column of $X$ contains the covariates for the $j^{th}$ sampling. $O =(O_{ij}) \in \mathcal{M}_{n \times N} (\mathbb{R})$ stands for the matrix of offsets, we consider that $O_{ij}=O^j_i$. \\
\\
This model enable us to take in account for fixed additional effects. Indeed, it can help to interpret the dependency parameters. For examples, 	Poisson log-normal model can be used to estimate an interaction network from the co-occurancy. With this model, we can hope, with a good choice of the covariables, that the dependencies encoded in the variance-covariance matrix $\Sigma$ will indicate the interraction between species.
\subsection{Properties}
\subsubsection{Density function of the Poisson log-normal distribution}
Following \cite{aitchison1989multivariate}, the Poisson log-normale distribution admits a density function given by :
\begin{center}
$\forall (m_1,...m_n) \in \mathbb{N}^n$, $h_{(\mu,\Sigma)}(m_1,...m_n)=\int_{\mathbb{R}^n} \prod_{i=1}^n f_{e^{\mu_i+z_i}}(m_i) g_{(0,\Sigma)}(z_1,...z_n) \mathrm{d}z_1...\mathrm{d}z_n$
\end{center}
Where $f_{\alpha}$ stands for the density function of a Poisson distribution with parameter $\alpha$, and $ g_{(0,\Sigma)}$ for the density function of a multivariate Normal distribution with parameters $(0,\Sigma)$.\\
\\
Taking in account covariates and offset, give the following distribution function :
\begin{center}
$ \forall X \in \mathbb{R}^d, \forall O \in \mathbb{R}^d, \forall (m_1,...m_n) \in \mathbb{N}^n$, $h_{(M,\Sigma \mid X, O)}(m_1,...m_n)=\int_{\mathbb{R}^n} \prod_{i=1}^n f_{e^{O_i+X^T \mu^i+z_i}}(m_i) g_{(0,\Sigma)}(z_1,...z_n) \mathrm{d}z_1...\mathrm{d}z_n$
\end{center}
\subsubsection{Moments of the Poisson log-normale distribution} Again, following \cite{aitchison1989multivariate}, the moments of the Poisson log-normale distribution can easily be obtained through conditional expectation. Considering the observation $Y^j_i$ is following a Poisson log-normal distribution with  parameters $(M , \Sigma)$ (with $\Sigma= (\sigma_{ij})$), the covariates $X \in \mathcal{M}_{d \times N} ( \mathbb{R})$ and the offset $O \in \mathcal{M}_{n \times N} (\mathbb{R})$ 
\begin{align*}
\mathbb{E}(Y^j_i)& =\mathrm{exp}(O_{ij}+ (X^j)^T\mu^i+\frac{1}{2}\sigma_{ii})\\
\mathbb{V}(Y^j_i)& =\mathrm{exp}(O_{ij}+(X^j)^T \mu^i+\frac{1}{2}\sigma_{ii})+(\mathrm{exp}(O_{ij}+(X^j)^T\mu^i+\frac{1}{2}\sigma_{ii})^2(\mathrm{exp}(\sigma_{ii})-1)\\
\mathrm{Cov}(Y^j_i,Y^j_k)& =\mathrm{exp}(O_{ij}+(X^j)^T \mu^i+\frac{1}{2}\sigma_{ii})\mathrm{exp}(O_{kj}+(X^j)^T \mu_k+\frac{1}{2}\sigma_{kk})(exp(\sigma_{ik}) - 1)
\end{align*}
\subsubsection{Overdispersion}
From the calculation of the moments, we see that for $Y^j_i$ following a Poisson log-normal distribution :
\begin{center}
$\mathbb{E}(Y^j_i) < \mathbb{V}(Y^j_i)$.   
\end{center}
The Poisson log-normal distribution is overdispersed, giving a clue that it can be applyed to a large range of multivariate count data, specially in ecology.
Indeed $\mathrm{Cov}(Y^j_i,Y^j_k)$ and $\sigma_{ik}$ have the same sign.
\subsection{Variational estimation of the parameters}
\textit{\`a travailler PLN/PCA}
\SR{}{Pour ce passage, il faut faire court en rappelant le principe de VEM et en disant qu'on sait le faire pour PLN.}
\section{Composite likelihood estimation}
\subsection{Definition and notation}
\subsubsection{Presentation of the composite likelihood}
One classical method to estimate the parameters of a distribution is to maximize a so called likelihood-function over all the parameters sets. The likelihood-function is typically the density of the modelled distribution taken over all observations :
\begin{center}
$\mathcal{L}_{(Y^1,..Y^N)}(M,\Sigma \mid X,O) = \prod_{j=1}^N h_{(M,\Sigma \mid X,O)}(Y^j)$
\end{center}
When the considered density function is in the exponential family, people often take the log-likelihood, expressed as follow : 
$\mathcal{L}_{(Y^1,..Y^N)}(M,\Sigma \mid X,O) = \sum_{j=1}^N \mathrm{log} (h_{(M,\Sigma \mid X,O)}(Y^j))$.\\
\\ 
In our case, the likelihood function is costly to calculate, because the dependency structure in the data implies to calculate integrals over $\mathbb{R}^n$. Several approached have been proposed in order to symplify the likelyhood function in the case of complex dependencies. We propose to use the composite likelihood approach (see\cite{varin2011overview}, \cite{pedeli2018pairwise} and \cite{varin2008composite}).\\
\\
Composite likelihood is expressed as a weighted product of the marginal or conditional densities.
\begin{definition}\cite{varin2011overview} Let $Y$ be a $n$-dimensional random vector, with density function $f_\theta$ parametrized by a $p$-dimensional unknown parameter $\theta \in \Theta$.\\
Let $\{\mathcal{A}_1,..., \mathcal{A}_k\}$ be a set of marginal or conditional events. We note $\mathcal{L}^k_{(y)} (\theta) \varpropto f_\theta(y)$ the associated (marginal or conditional) likelihood. The composite likelihood is then given by :
\begin{align*}
\mathcal{CL}_{(y)}(\theta) = \prod_{i=1}^k (\mathcal{L}^i_{(y)}(\theta))^{w_i} 
\end{align*}
Where $w_i$ are non negative weights to be choosen.
\end{definition}
\\
The idea behind composite marginal likelihood \cite{varin2008} is to compose low dimensional marginal densities in order to symplify the calculations, but also to capture the dependence between the parameters. \\
If there is no dependance structure, it is sufficient to take the product of the one-dimensional densities. Otherwise, the two-dimensionnal marginal densities are needed at least, to capture the  dependence between the parameters.\\
\\
No theoretical results exist about the loss of efficiency \cite{lele2006sampling}. The idea is to show that maximizing the composite likelihood is a consitent assymptotic estimator of the parameters.

\subsubsection{Composite likelihood for the PLN model}
In our case, by integration, we can show that the two-dimensional marginal density function is given by $ \forall (m_1,m_2) \in \mathbb{N}^2$,$\forall X \in \mathbb{R}^d$, $ \forall O \in \mathbb{R}^d$ :
\begin{center}
 $\sum_{(m_3...m_n) \in \mathbb{N}^{n-2}} h_{(\mu,\Sigma \mid X)}(m_1,...m_n) = \int_{\mathbb{R}^2} f_{e^{O_1+X^T \mu^1+z_1}}(m_1) f_{e^{O_2+X^T \mu^2+z_2}}(m_2) g_{(0,\Sigma^{(12)})}(z_1,z_2)\mathrm{d}z_1 \mathrm{d}z_2$.
\end{center}
with $\Sigma^{(12)}=\begin{pmatrix}
\sigma_{11} & \sigma_{12} \\
\sigma_{12} & \sigma_{22}\\
\end{pmatrix}$\\
With this simple expression of the pairwise density function, we can write the composite pairwise marginal density function given by : $\forall (Y^1,...,Y^N) \in (\mathbb{R}^n)^N$, $\forall O \in \mathcal{M}_{n \times d} (\mathbb{R})$, $\forall X \in \mathcal{M}_{d \times N}(\mathbb{R})$ :
\begin{align}
\mathcal{CL}_{(Y^1,...,Y^N,O)}(M,\Sigma \mid X) = \prod_{j=1}^N \prod_{1 \leq i < k \leq n}  h'_{(M^{(ik)},\Sigma^{(ik)} \mid X^j,O^j)}(Y^j_i,Y^j_k)
\end{align}
With $h'$ the pairwise density function defined as above , $M^{(ik)}$ the matrix constituted of the $i$-th and $k$-th row of M and  $\Sigma^{(ik)}  = \begin{pmatrix}
\sigma_{ii} & \sigma_{ik}\\
\sigma_{ik} & \sigma_{kk}\\
\end{pmatrix}$
For easier calculation, we take the log composite likelihood given by :
\begin{align*}
\mathcal{CL}_{(Y^1,...,Y^N,O)}(M,\Sigma \mid X) = \sum_{j=1}^N \sum_{1 \leq i < k \leq n}  \mathrm{log}(h'_{(M^{(ik)},\Sigma^{(ik)} \mid X^j,O^j)}(Y^j_i,Y^j_k))
\end{align*}
There exists some theoretical results about the consistency of the method of maximisation of the composite likelihood. In the next section we show that the pariwise log-composite likelihood, is an M-estimator and so all the theory developped for M-estimators \cite{vaart_1998} can be applied.
\subsection{Consistency}
\subsubsection{M-estimator theory}
\begin{definition}
Let $(Y^1,...Y^N) \in \mathcal{X}^N$ be a set of observations. Let $\theta$ be an unknown parameter. \\
$\overhat{\theta}_N(Y^1,...Y^N)$ is called an M-estimator of $\theta$ if it maximize a function :
\begin{align*}
\mathcal{M}_N : \theta \mapsto \frac{1}{N} \sum_{i=1}^N m_\theta(Y^i)
\end{align*}
with $m_\theta : \mathcal{X} \rightarrow \mathbb{R}$ a known function.
\end{definition}
\subsubsection{Consistency of the composite likelihood for the PLN model}
\textit{proof}
\subsection{Assymptotic normality}
\subsubsection{M-estimator theory}
\textit{chap.5 Assymptotic Statistics van der Vaart}
\subsubsection{Assymptotic normality of the composite likelihood for the PLN model}
\textit{proof}
\subsubsection{Construction of assymptotic statistic tests (?)}
\textit{Peut-etre trop scolaire...} \SR{}{Je suis d'accord. Il suffit de dire que la normalite asymptotique nous donne les intervalles de confiance (asymptotiques) et les tests (asymptotiques)}

\section{Composite likelihood inference for spatial data}
\subsection{Poisson log-normal model for spatial data}
\subsubsection{One specie\SR{}{s} \SR{(meme au singulier)}{}, spatial dependency}
\subsubsection{spatial parametrisation of the PLN}
\subsection{Computational optimisation by taking a sparce CL}

\section{Illustrations}
\subsection{Simulations}
\subsection{'Tiques' data}

\appendix
\section{Appendix}
\subsection{Gradients}
\subsection{Hessian}
\subsection{Spatial gradient and hessian}


\bibliographystyle{plain}
\bibliography{Biblio.bib}
\end{document}